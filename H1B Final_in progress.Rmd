---
title: "H1B Visa Final Report"
author: "Lo Yu Liu, Rohith Ramineni, Jessica Starck"
date: "April 8, 2020"
output: rmarkdown::html_vignette
---
  

```{r setup, include=TRUE, cache=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Overview 
  
Our analysis is around international work visas and the benefits to employers in predicting the outcome. The dataset contains information regarding the applications filed in the years 2015 to 2018 for applicants working in different sectors of industries in various countries. It includes details related to their employment opportunity and employer along with application information like date of submission and associated attorney information.  
While the data includes visa types applicable for the US, Australia, Singapore, and Chile, the majority of the applications are filed for the visa type H1-B which corresponds to the working visa permit in the United States of America for non-immigrant temporary workers. This visa type is sponsored by a U.S citizen employer and requires a minimum annual salary of $60,000 (as of 2019) for the applicant. Even then, there are some other important conditions to be satisfied to obtain the H1-B visa. Some of the important variables are given in the dataset along with their visa statuses. The four different outcomes in the visa statuses are: 

- **Certified:** The visa is granted 

- **Denied:** The visa is rejected  

- **Certified-Withdrawn:** The visa is granted but the application is withdrawn by the employer for some reason unspecified.  

- **Withdrawn:** The application is withdrawn before the decision is finalized.  

The objective of this predictive modeling is to create a model such that based on the given variables, the outcome of the visa status is predicted by selecting the relevant factors. This helps the employer (client) to make business decisions regarding the sponsoring of applicants for H1-Bs. For example, based on the confidence in the certification of the applicant, the employer can begin planning and preparing projects accordingly as well as proactively adjust staffing levels as needed.
 

<br><br>

## Methodology  

The dataset includes applications from 2015 to 2018 with around 50 variables.  After exploratory analysis, it was clear that the applications, and therefore required data entry fields, have changed overtime and additional pre-processing of the data was required prior to conducting any predictive modeling. This process included removal of rows with too many missing values (i.e. no information related to that column) and columns which either were not relevant or included little variation in their values. Some variables, such as phone numbers, cities, job title, etc. had too many levels and are not practically relevant to the status of the visa. Other variables, variables that are only relevant to “completed” applications, such as DECISION_DATE were removed as they would not be available variables for in-progress applications. Lastly, variables like EMPLOYMENT_START_DATE, EMPLOYMENT_END_DATE, or CASE_SUBMITTED (date), are time-based variables that that were transformed into useable variables applicable to future applications. For example, a new variable was created for the employment duration (in months) and the CASE_SUBMITTED date was reduced to only the submitted month to evaluate the seasonal influence. Another transformation we had to make was regarding the pay rate for the applicant. In the years 2017 and 2018, the wages were given in the form of a range, ‘WAGE_RATE_OF_PAY_FROM’ and ‘WAGE_RATE_OF_PAY_TO’. We calculated the average of these two values as the salary of the applicant and a new variable ‘WAGE_MEAN’ was created; replacing the above two variables. Lastly, the wage rates were given in hourly, weekly, biweekly, and yearly basis so they all were converted into yearly rates to standardize the unit of measure. The remaining variables and a summary of the values can be found in Figures 3 and 4 in Appendix A.  

After the cleaning of the dataset is done, all the variables are converted into appropriate units such as character, factor, etc. and the dataset is split into training and testing subsets. The outcome variable CASE_STATUS has four outcomes as mentioned above which were converted into two results for the model to become a binary classification problem. The following are the two options considered for this conversion.

> **Option-1:** The ‘Certified_Withdrawn’ is considered to be in ‘Certified’ case status i.e., it is assumed that the visa is granted if the outcome is ‘Certified_Withdrawn’ and ‘Withdrawn’ is considered to be ‘Denied’. Hence the four outcomes of the case status are converted into two: either certified or denied.

> **Option-2:** Here the outcomes are classified into two: either successful or unsuccessful. The ‘Certified’ case status is considered successful and the rest outcomes go into the unsuccessful category leaving us with only two outcomes.  

From the standpoint of the client (the employer), it is most desired to know when the outcome results in an employee that is working for the client. With the assumption that withdrawn applications mean the applicant is no longer allowed to be employed, we chose Option 2 from above to best align with our clients needs. Five classification methods of different types were chosen to offer a variety of approaches and support for complexity in the data. The five models used are Logistic regression, PLS, Regression tree, Support Vector Machines and Random Forest classifier. Cross validation with ten folds is performed for the dataset for each of the models to increase the effectiveness of each observation in the dataset. To determine the best fit, the accuracy is compared through the ROC metric and the sensitivity and specificity for each. 


<br><br>

## Results  

After cleaning the dataset by following the above steps and segregating the CASE_STATUS into two outcomes, the distribution between successful and unsuccessful classifications is 88% and 12%, respectively. The division between CASE_STATUS before and after re-classification can be seen in Figure 1 and Figure 2 in Appendix A.  The accuracy of all models are very close ranging from 89.9% to 91.3%. The certainty for each model (i.e, true positive fraction with respect to false positive fraction for each fold) can be seen in Figure 6 while Figure 7 shows the comparison of the average ROC for all models. It is observed that all the models are pretty close which can be attributed to its accuracy.  A comparison of ROC, sensitivity and specificity is done by plotting the graph with a confidence level of 95% which can be seen in Figure 5. 



<br><br>


## Critical Assessment  

One concern we have is that our dataset may not be exhaustive of all H-1B applicants, as it is found that only 1% of the total applications had a status of denied whereas further research from other sources has shown around 9% is the denial percentage for the H1-B visa over the years 2015 to 2019. We are also aware that the large discrepancy between the percent certified and not can bias the results towards the certified visa statuses. Also it is assumed that “Withdrawn case status” is considered to be negative for the client and hence is added to the unsuccessful category, but the true reason and outcome of the withdrawn application is unknown. There may be possible circumstances where an application is withdrawn because the employee has other means to support their work with the client. Reasons for the ‘certified-withdrawn’ and ‘withdrawn’ applications can help improve the efficiency of this model as the reason helps to sort the data more clearly and effectively into successful and unsuccessful case outcomes. Lastly, due to the nature of a federal program such as the H-1B visas and the inherent relationship between international countries, our predictive model will be unable to reflect any future policy or process changes relating to this program. 


<br><br>

## Appendix A  
  
```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(readr)
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(DMwR)
library(knitr)
library(tibble)
library(tidyr)
library(lubridate)
library(rpart)
library(rpart.plot)
library(glmnet)
library(kernlab)
library(randomForest)
library(plotROC)

```



```{r load_data, message=FALSE, echo=FALSE}
#load("H1BPrediction.Rdata")
#load("all_years.Rdata")  ## dataset we created  and organized for the EDA
load("H1BVisaClassSuccessful-2.RData")
load("H1BFinalData.RData")

```

```{r variables, message=FALSE, echo=FALSE, eval=FALSE}
names(all_years_2)
```

<br><br>
**Figure 1:** Distribution of Original CASE_STATUS  

```{r general_plots, message=FALSE, echo=FALSE, fig.align="left", fig.width=8, fig.height=6}
## case_statuses' percentage among all applications
all_years_2 %>%
  group_by(CASE_STATUS) %>%
  summarise(perc = n()/nrow(all_years_2)) %>%
  ggplot(mapping = aes(x = CASE_STATUS, y = perc, fill = CASE_STATUS,
                       label = scales::percent(perc))) +
  geom_bar(stat= "identity") +
  geom_text(position = position_dodge(width = .9),    # move to center of bars
            vjust = -0.5,    # nudge above top of bar
            size = 4) +
  scale_y_continuous(labels = scales::percent)
```


```{r clean_data, message=FALSE, echo=FALSE, eval=FALSE}
## remove rows with over 0.2% missing values
FY_new <- all_years_2[-manyNAs(all_years),]

FY_new <- mutate(FY_new, SOC_CATEGORY = (substr(FY_new$SOC_CODE, start = 1, stop = 2)))


## Convert variables to numeric/integer/factor for tuning models
#FY_new$PREVAILING_WAGE <- as.integer(gsub(",", "", FY_new$PREVAILING_WAGE))
#FY_new$WAGE_RATE_OF_PAY <- as.numeric(gsub(",", "", FY_new$WAGE_RATE_OF_PAY))  # ADDED THIS LINE. NEED TO CONSIDER WAGE_RATE_OF_PAY
FY_new$WAGE_RATE_OF_PAY_TO <- as.numeric(gsub(",", "", FY_new$WAGE_RATE_OF_PAY_TO))
FY_new$WAGE_RATE_OF_PAY_FROM <- as.numeric(gsub(",", "", FY_new$WAGE_RATE_OF_PAY_FROM))
#FY_new$SOC_CODE <- as.factor(gsub(".00", "", FY_new$SOC_CODE))
FY_new$year <- as.integer(FY_new$year)
FY_new$NAICS_CODE <- as.integer(FY_new$NAICS_CODE)


FY_new <- mutate(FY_new, WAGE_MEAN = (WAGE_RATE_OF_PAY_FROM + WAGE_RATE_OF_PAY_TO)/2)

FY_new <- mutate(FY_new, WAGE_RATE_YEARLY = ifelse(WAGE_UNIT_OF_PAY == "Year", WAGE_MEAN,
                                            ifelse(WAGE_UNIT_OF_PAY == "Month", WAGE_MEAN*12,
                                            ifelse(WAGE_UNIT_OF_PAY == "Week", WAGE_MEAN*52, 
                                            ifelse(WAGE_UNIT_OF_PAY == "Bi-Weekly", WAGE_MEAN*52/2, WAGE_MEAN*40*52)))))


```

```{r, message=FALSE, echo=FALSE, eval=FALSE}
names(FY_new)
```


```{r, message=FALSE, echo=FALSE, eval=FALSE}
#can remove SOC_CODE, and "wage" variables except for WAGE_RATE_YEARLY

## nearZeroVar(FY_new) 
## romove variables with near zero variance


names(FY_new[c(3,4,5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 19,20, 21, 24, 25, 26,27, 28, 29,30,32, 34, 35, 36, 37, 42, 49, 52)])

FY_new2 <- FY_new[, -c(3,4,5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 19,20, 21, 24, 25, 26,27, 28, 29,30,32, 34, 35, 36, 37, 42, 49, 52)]
nzv_FY_new2 <- nearZeroVar(FY_new2, saveMetrics = TRUE)
nzv_FY_new2 <- rownames_to_column(nzv_FY_new2)
nzv_FY_new2 <- nzv_FY_new2 %>%
 filter(nzv == TRUE)

FY_new2 <- FY_new2 %>%
  select(-c(nzv_FY_new2$rowname))

FY_new2$CASE_SUBMITTED <- month(FY_new2$CASE_SUBMITTED)

FY_new2$year <- as.integer(FY_new2$year)
FY_new2$PW_SOURCE_YEAR <- as.integer(FY_new2$PW_SOURCE_YEAR)
FY_new2$SOC_CATEGORY <- as.integer(FY_new2$SOC_CATEGORY)

FY_new2 <- FY_new2 %>%
  filter(PW_SOURCE_YEAR >1000)
```


```{r, message=FALSE, echo=FALSE, eval=FALSE}
## OPTION 1

## Change the reposnse to binary outcomes

FY_new3 <- FY_new2 %>%
  filter(CASE_STATUS != "WITHDRAWN")

FY_new3$CASE_STATUS <- gsub("CERTIFIED", "CERTIFIED", FY_new3$CASE_STATUS)
FY_new3$CASE_STATUS <- gsub("DENIED", "DENIED", FY_new3$CASE_STATUS)
FY_new3$CASE_STATUS <- gsub("CERTIFIED-WITHDRAWN", "CERTIFIED", FY_new3$CASE_STATUS)
#FY_new2$CASE_STATUS <- gsub("WITHDRAWN", "DENIED", FY_new2$CASE_STATUS)

FY_new3$CASE_STATUS <- as.factor(FY_new3$CASE_STATUS)

FY_new3 <- FY_new3 %>%
  select(-SUPPORT_H1B)
```


```{r, message=FALSE, echo=FALSE, eval=FALSE}
## Option 2

## Change the reposnse to binary outcomes

#interesting find: because gsub is looking for patterns, it included "CERTIFIED-WITHDRAWN" in the initial "CERTIFIED" script. Then it also added the "WITHDRAWN" part so the result was a new category of "SUCCESSFUL-UNSUCCESSFUL"... so to fix this, I had to rearrange for "CERTIFIED-WITHDRAWN" to be evaluated first.

FY_new3 <- FY_new2

FY_new3$CASE_STATUS <- gsub("CERTIFIED-WITHDRAWN", "UNSUCCESSFUL", FY_new3$CASE_STATUS)
FY_new3$CASE_STATUS <- gsub("CERTIFIED", "SUCCESSFUL", FY_new3$CASE_STATUS)
FY_new3$CASE_STATUS <- gsub("DENIED", "UNSUCCESSFUL", FY_new3$CASE_STATUS)
FY_new3$CASE_STATUS <- gsub("WITHDRAWN", "UNSUCCESSFUL", FY_new3$CASE_STATUS)

FY_new3$CASE_STATUS <- as.factor(FY_new3$CASE_STATUS)

FY_new3 <- FY_new3 %>%
  select(-SUPPORT_H1B)

```

<br><br> 

  
**Figure 2:** Distribution of revised CASE_STATUS Classifications

```{r revised_plots, message=FALSE, echo=FALSE, fig.align="left", fig.width=8, fig.height=6}
## case_statuses' percentage among all applications (after cleaning)
FY_new3 %>%
  group_by(CASE_STATUS) %>%
  summarise(perc = n()/nrow(FY_new3)) %>%
  ggplot(mapping = aes(x = CASE_STATUS, y = perc, fill = CASE_STATUS,
                       label = scales::percent(perc))) +
  geom_bar(stat= "identity") +
  geom_text(position = position_dodge(width = .9),    # move to center of bars
            vjust = -0.5,    # nudge above top of bar
            size = 4) +
  scale_y_continuous(labels = scales::percent)
```
<br><br>  

**Figure 3:** Final variables and data types used in models  

```{r, message=FALSE, echo=FALSE}

str(FY_new3)

```


```{r datasplit, message=FALSE, echo=FALSE, eval=FALSE}
index <- sample.int(n = nrow(FY_new3),
                    size = floor(0.008* nrow(FY_new3)),
                    replace = FALSE)

train_FY <- FY_new3[index,]
test_FY <- FY_new3[-index,]

library(janitor)
train_FY <- clean_names(train_FY)
test_FY <- clean_names(test_FY)
```
<br><br>   

**Figure 4:** Training Dataset Summary 

```{r, message=FALSE, echo=FALSE, fig.cap= "Summary of Training Data"}
#dim(train_FY)

summary(train_FY)
```

```{r trControl, message=FALSE, echo=FALSE, eval=FALSE}
ctrl <- trainControl(method = "cv",
                     number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
```

```{r glmnet, warning=FALSE, eval=FALSE, echo=FALSE}
#### Linear: GLMnet 

enet.grid <- expand.grid(alpha = c(0.1, 0.3, 0.5, 0.7),
                         lambda = exp(seq(-6, 1, length.out = 21)))

set.seed(4321)
fit.glmnet <- train(case_status ~ ., data = train_FY,
                    method = "glmnet",
                    metric = "ROC",
                    tuneGrid = enet.grid,
                    preProcess = c("center", "scale"),
                    trControl = ctrl, na.action = na.omit)
```
```{r, echo=FALSE, eval=FALSE}
plot(fit.glmnet, xTrans = log)

## confusion matrix
confusionMatrix.train(fit.glmnet)
```

```{r pls, warning=FALSE, eval=FALSE, echo=FALSE}
## Linear: PLS  

library(pls)
pls_grid <- expand.grid(ncomp = seq(1, 13, by = 1))

set.seed(4321)
fit.pls <- train(case_status ~ ., data = train_FY,
                 method = "pls",
                 metric = "ROC",
                 tuneGrid = pls_grid,
                 preProcess = c("center", "scale"),
                 trControl = ctrl, na.action = na.omit)
```
```{r, echo=FALSE, eval=FALSE}
fit.pls

## plot
plot(fit.pls)

## confusion matrix
confusionMatrix.train(fit.pls)
```

```{r tree, echo=FALSE, eval=FALSE}
### Tree

library(rpart)
library(rpart.plot)

fit.tree <- train(x = train_FY[, 2:19],
                  y = train_FY$case_status,
                  method = "rpart",
                  metric = "ROC",
                  tuneLength = 30,
                  trControl = ctrl, na.action = na.omit)
```
```{r, echo=FALSE, eval=FALSE}
fit.tree

## plot
prp(fit.tree$finalModel,
    box.palette = "Reds",
    tweak = 1.5,
    varlen = 30)

## confusion matrix
confusionMatrix.train(fit.tree)

```

```{r nonlinear, echo=FALSE, eval=FALSE}
### Nonlinear: SVM

set.seed(4321)
fit.svm <- train(case_status ~ ., data = train_FY,
                 method = "svmRadial",
                 metric = "ROC",
                 preProcess = c("center", "scale"),
                 trControl = ctrl, na.action = na.omit)
```
```{r, echo=FALSE, eval=FALSE}
fit.svm

## the best model 
fit.svm$bestTune

## confusion matrix
confusionMatrix.train(fit.svm)
```

```{r ensemble, echo=FALSE, eval=FALSE}
### Ensemble: Random Forest

rf_gird <- expand.grid(mtry = c(2, 3, 6, 9, 12, 15))

set.seed(4321)
fit.rf <- train(case_status ~ ., data = train_FY,
                method = "rf",
                metric = "ROC",
                trControl = ctrl,
                tuneGrid = rf_gird,
                importance = TRUE, na.action = na.omit)
```
```{r, echo=FALSE, eval=FALSE}
fit.rf

## confusion matrix
confusionMatrix.train(fit.rf)
```


```{r combineresults, echo=FALSE, fig.align="left", fig.width=8, fig.height=6, eval=FALSE}
#**Figure 5:** Model Comparision (ROC, Sensitivity, Specificity) 

model_cv_result <- resamples(list(GLMNET = fit.glmnet,
                                  PLS = fit.pls,
                                  SVM = fit.svm,
                                  RF = fit.rf,
                                  Tree = fit.tree))
## visualize ROC, Sensitivity, and Specificity comparison across models
dotplot(model_cv_result)
```

```{r option2, echo=FALSE, eval=FALSE}
cv_pred_results <- fit.glmnet$pred %>% tbl_df() %>%
  filter(alpha %in%  fit.glmnet$bestTune$alpha,
         lambda %in% fit.glmnet$bestTune$lambda) %>%
  select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
  mutate(model_name = "GLMNET") %>%
  bind_rows(fit.pls$pred %>%
              tbl_df() %>%
              filter(ncomp %in% fit.pls$bestTune$ncomp) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "PLS")) %>%
  bind_rows(fit.svm$pred %>%
              tbl_df() %>%
              filter(sigma %in% fit.svm$bestTune$sigma,
                     C %in% fit.svm$bestTune$C) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "SVM")) %>%
  bind_rows(fit.rf$pred %>%
              tbl_df() %>%
              filter(mtry == fit.rf$bestTune$mtry) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "RF")) %>%
  bind_rows(fit.tree$pred %>%
              tbl_df() %>%
              filter(cp == fit.tree$bestTune$cp) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "Tree"))
```

```{r foldscomparison, echo=FALSE, fig.align="left", fig.width=8, fig.height=6, eval=FALSE}
#**Figure 6:** Model Comparision (ROC for each 10 Fold) 

## plot the ROC for different folds
cv_pred_results %>%
  ggplot(mapping = aes(m = SUCCESSFUL,
                       d = ifelse(obs == "SUCCESSFUL", 1, 0))) +
  geom_roc(cutoffs.at = 0.5,
           mapping = aes(color = Resample)) +
  geom_roc(cutoffs.at = 0.5) +
  coord_equal() +
  facet_wrap(~ model_name) +
  style_roc()
```

```{r modelcomparison, echo=FALSE, fig.align="left", fig.width=8, fig.height=6, eval=FALSE}
#**Figure 7:** Model Comparision (Average ROC) 

cv_pred_results %>%
  ggplot(mapping = aes(m = SUCCESSFUL, 
                       d = ifelse(obs == "SUCCESSFUL", 1, 0),
                       color = model_name)) +
  geom_roc(cutoffs.at = 0.5) +
  coord_equal() +
  style_roc() +
  ggthemes::scale_color_colorblind()
```

```{r plsvarimp, echo=FALSE, eval=FALSE}
plot(varImp(fit.pls), top = 25)
```

```{r organize testset, echo=FALSE, eval=FALSE}
test_FY <- na.omit(test_FY) ## remove rows with NAs
```

```{r pred_rf, echo=FALSE, eval=FALSE}
pred <- predict(fit.rf, test_FY)
prediction <- as.data.frame(pred)
pred_cb <- cbind(prediction, test_FY)

ggplot(pred_cb, mapping = aes(x = case_status)) +
  geom_density(color = "black", size = 1) +
  geom_density(aes(x = pred), color = "tomato", size = 1)
```


Choose Best Parameters
========================
(finding simpliest parameters that have an ROC within one Standard Deviation from the maximum ROC)


```{r glmnet_best, echo=FALSE, warning=FALSE, message = FALSE, eval=FALSE}
#**GLMnet**

glmnet_num <- fit.glmnet$results %>%
 filter(ROC == max(ROC)) %>%
summarize(n = ROC-ROCSD) %>%
 as.numeric(n)

bestglmnet <- fit.glmnet$results %>%
filter(ROC >= glmnet_num) %>%
 filter(alpha == min(alpha)) %>%
 filter(lambda == min(lambda))

bestglmnet$model <- "GLMNET"

glmnetvalues <- bestglmnet %>%
  select(model, ROC, Sens, Spec, alpha, lambda)

```
 

```{r pls_best, echo=FALSE, warning=FALSE, message = FALSE, eval=FALSE}
#**PLS** 

pls_num <- fit.pls$results %>%
  filter(ROC == max(ROC)) %>%
  summarize(n = ROC-ROCSD) %>%
  as.numeric(n)

bestpls <- fit.pls$results %>%
filter(ROC >= pls_num) %>%
  filter(ncomp == min(ncomp))

bestpls$model <- "PLS"

plsvalues <- bestpls %>%
  select(model, ROC, Sens, Spec, ncomp)
```
  
  
```{r rpart_best, echo=FALSE, warning=FALSE, message = FALSE, eval=FALSE}
#**Regression Tree**

rpart_num <- fit.tree$results %>%
  filter(ROC == max(ROC)) %>%
  filter(cp == max(cp)) %>%
  summarize(n = (ROC-ROCSD)) %>%
  as.numeric(n)

besttree <- fit.tree$results %>%
filter(ROC >= rpart_num) %>%
  filter(cp == max(cp))

besttree$model <- "Tree"

treevalues <- besttree %>%
  select(model, ROC, Sens, Spec, cp)

```
  
  
```{r mars_best, echo=FALSE, warning=FALSE, message = FALSE, eval=FALSE}
#**SVM**

svm_num <- fit.svm$results %>%
  filter(ROC == max(ROC)) %>%
  summarize(n = ROC-ROCSD) %>%
  as.numeric(n)

bestsvm <- fit.svm$results %>%
filter(ROC >= svm_num) %>%
  filter(C == min(C))

bestsvm$model <- "SVM"

svmvalues <- bestsvm %>%
  select(model, ROC, Sens, Spec, sigma, C)

```

  
```{r rf_best, echo=FALSE, warning=FALSE, message = FALSE, eval=FALSE}
#**RF**

rf_num <- fit.rf$results %>%
  filter(ROC == max(ROC)) %>%
  summarize(n = ROC-ROCSD) %>%
  as.numeric(n)

bestrf <- fit.rf$results %>%
filter(ROC >= rf_num) %>%
  filter(mtry == min(mtry))

bestrf$model <- "RF"

rfvalues <- bestrf %>%
  select(model, ROC, Sens, Spec, mtry)
```
  

<br>

Create new model with chosen parameters
=============================================


```{r glmnet2, warning=FALSE, echo=FALSE, eval=FALSE}
#### Linear: GLMnet 

set.seed(4321)
final.glmnet <- train(case_status ~ ., data = train_FY,
                      method = "glmnet",
                      metric = "ROC",
                      tuneGrid = data.frame(alpha = glmnetvalues$alpha, 
                                            lambda = glmnetvalues$lambda),
                      preProcess = c("center", "scale"),
                      trControl = ctrl, na.action = na.omit)

```
```{r, echo=FALSE}

final.glmnet

## confusion matrix
confusionMatrix.train(final.glmnet)
```

```{r pls2, warning=FALSE, echo=FALSE, eval=FALSE}
## Linear: PLS  

set.seed(4321)
final.pls <- train(case_status ~ ., data = train_FY,
                 method = "pls",
                 metric = "ROC",
                 tuneGrid = expand.grid(ncomp=plsvalues$ncomp),
                 preProcess = c("center", "scale"),
                 trControl = ctrl, na.action = na.omit)

```
```{r, echo=FALSE}
final.pls

## confusion matrix
confusionMatrix.train(final.pls)
```

```{r tree2, echo=FALSE, eval=FALSE}
### Tree

final.tree <- train(case_status ~., data = train_FY,
                  method = "rpart",
                  metric = "ROC",
                  tuneGrid = expand.grid(cp=treevalues$cp),
                  trControl = ctrl, na.action = na.omit)
```
```{r, echo=FALSE,  fig.align="left", fig.width=8, fig.height=6}
final.tree

## plot
prp(final.tree$finalModel,
    box.palette = "Reds",
    tweak = 1.5,
    varlen = 30)

## confusion matrix
confusionMatrix.train(final.tree)

```

```{r svm2, echo=FALSE, message=FALSE, eval=FALSE}
### Nonlinear: SVM

#doesnt like no variance in each state, etc.

set.seed(4321)
final.svm <- train(case_status ~ ., data = train_FY,
                 method = "svmRadial",
                 metric = "ROC",
                 tuneGrid = expand.grid(sigma = svmvalues$sigma, C = svmvalues$C),
                 preProcess = c("center", "scale"),
                 trControl = ctrl, na.action = na.omit)

```
```{r, echo=FALSE}
final.svm


## confusion matrix
confusionMatrix.train(final.svm)
```

```{r randforest2, echo=FALSE, eval=FALSE}
### Ensemble: Random Forest

set.seed(4321)
final.rf <- train(case_status ~ ., data = train_FY,
                method = "rf",
                metric = "ROC",
                trControl = ctrl,
                tuneGrid = expand.grid(mtry=rfvalues$mtry),
                importance = TRUE, na.action = na.omit)

```
```{r, echo=FALSE}
final.rf

## confusion matrix
confusionMatrix.train(final.rf)
```


 
Predict on Test Data
======================

```{r, echo=FALSE, eval=FALSE}

set.seed(123)
idxsmall <- sample.int(n = nrow(test_FY),
                    size = 36152,
                    replace = FALSE)

test_FY_small <- test_FY[idxsmall,]

predictions <- test_FY_small
pred_probs <- test_FY_small
```

```{r}
test_FY_small %>%
  group_by(case_status) %>%
  summarize(n=n())

```

```{r predict, warning=FALSE, echo=FALSE}

predictions$glmnet <- predict(final.glmnet, test_FY_small)
predictions$pls <- predict(final.pls, test_FY_small)
predictions$tree <- predict(final.tree, test_FY_small)
predictions$svm <- predict(final.svm, test_FY_small)
predictions$rf <- predict(final.rf, test_FY_small)

predictions <- predictions %>%
  select(case_status, glmnet:rf)
```

#predictions for probabilities  
(not sure how to format this. This means there has to be two columns for every model (one SUCCESSFUL and one UNSUCCESSFUL), need a different table or maybe cbind?)

pred_probs$glmnet <- predict(final.glmnet, test_FY_small, type='prob')
pred_probs$pls <- predict(final.pls, test_FY_small, type='prob')
pred_probs$tree <- predict(final.tree, test_FY_small, type='prob')
pred_probs$svm <- predict(final.svm, test_FY_small, type='prob')
#pred_probs$rf <- predict(final.rf, test_FY_small, type='prob')

pred_probs <- pred_probs %>%
  select(case_status, glmnet:svm)




ROC plots - Pick best model
============================

```{r combineresults2, echo=FALSE}
final_model_results <- resamples(list(GLMNET = final.glmnet,
                                  PLS = final.pls,
                                  SVM = final.svm,
                                  RF = final.rf,
                                  Tree = final.tree))
```
```{r, echo=FALSE,  fig.align="left", fig.width=8, fig.height=6}
## visualize ROC, Sensitivity, and Specificity comparison across models
dotplot(final_model_results)
```


```{r, echo=FALSE, eval=FALSE}

#changed to make for final.models

final_cv_pred_results <- final.glmnet$pred %>% tbl_df() %>%
  filter(alpha %in%  final.glmnet$bestTune$alpha,
         lambda %in% final.glmnet$bestTune$lambda) %>%
  select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
  mutate(model_name = "GLMNET") %>%
  bind_rows(final.pls$pred %>%
              tbl_df() %>%
              filter(ncomp %in% final.pls$bestTune$ncomp) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "PLS")) %>%
  bind_rows(final.svm$pred %>%
              tbl_df() %>%
              filter(sigma %in% final.svm$bestTune$sigma,
                     C %in% final.svm$bestTune$C) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "SVM")) %>%
  bind_rows(final.tree$pred %>%
              tbl_df() %>%
              filter(cp == final.tree$bestTune$cp) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "Tree"))
  bind_rows(final.rf$pred %>%
              tbl_df() %>%
              filter(mtry == final.rf$bestTune$mtry) %>%
              select(pred, obs, SUCCESSFUL, UNSUCCESSFUL, rowIndex, Resample) %>%
              mutate(model_name = "RF"))
```
<br><br>

**Figure 6:** Model Comparision (ROC for each 10 Fold) 

```{r foldscomparison2, echo=FALSE, fig.align="left", fig.width=8, fig.height=6}
## plot the ROC for different folds
final_cv_pred_results %>%
  ggplot(mapping = aes(m = SUCCESSFUL,
                       d = ifelse(obs == "SUCCESSFUL", 1, 0))) +
  geom_roc(cutoffs.at = 0.5,
           mapping = aes(color = Resample)) +
  geom_roc(cutoffs.at = 0.5) +
  coord_equal() +
  facet_wrap(~ model_name) +
  style_roc()


## different cutoff
final_cv_pred_results %>%
  ggplot(mapping = aes(m = SUCCESSFUL,
                       d = ifelse(obs == "SUCCESSFUL", 1, 0))) +
  geom_roc(cutoffs.at = 0.7,
           mapping = aes(color = Resample)) +
  geom_roc(cutoffs.at = 0.7) +
  coord_equal() +
  facet_wrap(~ model_name) +
  style_roc()

```

<br><br>

**Figure 7:** Model Comparision (Average ROC) 

```{r modelcomparison2, echo=FALSE, fig.align="left", fig.width=8, fig.height=6}
final_cv_pred_results %>%
  ggplot(mapping = aes(m = UNSUCCESSFUL, 
                       d = ifelse(obs == "UNSUCCESSFUL", 1, 0),
                       color = model_name)) +
  geom_roc(cutoffs.at = 0.5) +
  coord_equal() +
  style_roc() +
  ggthemes::scale_color_colorblind()
```




```{r}
## this look like a step change generally, thus, this can be an ideal ROC curve
ggplot(fit.tree$results, mapping = aes(x = 1 - Spec, y = Sens)) +
  geom_line()
```
```{r}
## Get the ROC curve
roc.tree <- roc(test_FY$case_status, 
            predict(fit.tree, test_FY, type = "prob")[,1], 
            levels = rev(levels(test_FY$case_status)))
roc.tree ## AUC = 0.6747
plot(roc.tree, print.thres = c(.5), type = "S",
     print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
     print.thres.cex = .8, 
     legacy.axes = TRUE,
     main = "ROC of tree")

roc.rf <- roc(test_FY$case_status, 
            predict(fit.rf, test_FY, type = "prob")[,1], 
            levels = rev(levels(test_FY$case_status)))
roc.rf ## AUC = 0.7433  highest
plot(roc.rf, print.thres = c(.5), type = "S",
     print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
     print.thres.cex = .8, 
     legacy.axes = TRUE,
     main = "ROC of rf")

roc.glmnet <- roc(test_FY$case_status, 
            predict(fit.glmnet, test_FY, type = "prob")[,1], 
            levels = rev(levels(test_FY$case_status)))
roc.glmnet ## AUC = 0.7054
plot(roc.glmnet, print.thres = c(.5), type = "S",
     print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
     print.thres.cex = .8, 
     legacy.axes = TRUE,
     main = "ROC of glmnet")

roc.pls <- roc(test_FY$case_status, 
            predict(fit.pls, test_FY, type = "prob")[,1], 
            levels = rev(levels(test_FY$case_status)))
roc.pls ## AUC = 0.706
plot(roc.pls, print.thres = c(.5), type = "S",
     print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
     print.thres.cex = .8, 
     legacy.axes = TRUE,
     main = "ROC of pls")


roc.svm <- roc(test_FY$case_status, 
            predict(fit.svm, test_FY, type = "prob")[,1], 
            levels = rev(levels(test_FY$case_status)))
roc.svm ## AUC = 0.6937
plot(roc.svm, print.thres = c(.5), type = "S",
     print.thres.pattern = "%.3f (Spec = %.2f, Sens = %.2f)",
     print.thres.cex = .8, 
     legacy.axes = TRUE,
     main = "ROC of svm")



```
```{r}
## pick the median predicted value of the Successful cases for the test set
## this will be close to the threshold when we using ROC curve where TPR&FPR overlap
pred_rf <- predict(fit.rf, test_FY, type = "prob")
summary(pred_rf$SUCCESSFUL)
```

```{r}
matplot(data.frame(roc.rf$sensitivities, roc.rf$specificities),
        x = roc.rf$thresholds,
        xlab = "threshold", ylab = "TPR, TNR")
legend('bottomright', legend=c('TPR', 'TNR'), lty=1:2, col=1:2)
```
```{r}
threshold_5 <- 0.5
predicted_values5 <- ifelse(predict(final.rf, test_FY_small, type='prob') > threshold_5, 1, 0)
actual_values <- test_FY_small$case_status
conf_matrix_5 <- table(predicted_values5, actual_values)
conf_matrix_5
```

```{r}
threshold_8 <- 0.8
predicted_values8 <- ifelse(predict(final.rf, test_FY_small, type='prob') > threshold_8, 1, 0)
actual_values <- test_FY_small$case_status
conf_matrix_8 <- table(predicted_values8, actual_values)
conf_matrix_8
```


```
{r, eval=FALSE}
#save_list = list("final.glmnet", "final.pls", "final.svm", "final.tree", "predictions", "test_FY_small")

save("final.glmnet", "final.pls", "final.svm", "final.tree", "final.rf", "predictions", "test_FY_small", "final_cv_pred_results", "all_years_2", "FY_new3", "train_FY", file = "H1BFinalData.RData")
```



```
{r, echo=FALSE, eval=FALSE}
#save.image(file = "H1BVisaClassSuccessful-2.RData")
#save.image(file = "H1BVisaClassCertified.RData")
save.image(file = "H1BFinalAll.RData")

```
